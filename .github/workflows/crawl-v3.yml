name: UIUC Crawler v3

on:
  # Manual trigger for testing
  workflow_dispatch:

permissions:
  contents: write

jobs:
  crawl-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Preserve coordinates.csv
        run: cp ./apps/crawler-v3/data/coordinates.csv /tmp/coordinates.csv

      - name: Checkout gh-pages branch to data folder (if exists)
        uses: actions/checkout@v4
        continue-on-error: true
        with:
          ref: gh-pages
          path: ./apps/crawler-v3/data

      - name: Restore coordinates.csv
        run: cp /tmp/coordinates.csv ./apps/crawler-v3/data/coordinates.csv
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: apps/crawler-v3/package-lock.json
      
      - name: Install dependencies
        working-directory: ./apps/crawler-v3
        run: npm ci

      - name: Clean old subject cache (force fresh scrape)
        working-directory: ./apps/crawler-v3/data
        run: |
          # Remove old subject directories to force fresh scrape with fixed index remapping
          rm -rf */subjects */progress.json
          echo "Cleaned old subject caches"

      - name: Run crawler
        working-directory: ./apps/crawler-v3
        env:
          COURSES_PER_SUBJECT: 1
          MAX_SUBJECTS: 10
        run: npm start
      
      - name: Deploy to GitHub Pages
        uses: JamesIves/github-pages-deploy-action@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: gh-pages
          folder: ./apps/crawler-v3/data
          clean: false
          commit-message: 'Update UIUC course data'
